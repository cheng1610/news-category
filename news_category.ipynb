{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheng1610/news-category/blob/main/news_category.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gUj8a4dkVYZ"
      },
      "source": [
        "# 基於深度學習的新聞主題自動分類系統\n",
        "### 1. 專案介紹&簡介\n",
        "- 本專案開發一個自動判斷新聞主題的系統，可以將新聞進行多分類，例如：\n",
        "  - World：國際新聞\n",
        "  - Sports：體育新聞\n",
        "  - Business：商業與經濟新聞\n",
        "  - Sci/Tech：科技與科學新聞\n",
        "- **開發環境**：Google Colab\n",
        "- **深度學習框架**：TensorFlow\n",
        "- **資料集**：使用包含在TensorFlow Datasets中的AG News Dataset (一個經典的英文新聞分類資料集，常用於自然語言處理和文本分類的研究與測試)\n",
        "- **資料量**：訓練集共120,000篇文章，測試集共7,600 篇文章\n",
        "- **資料結構**：訓練集以及測試集的每筆資料(文章)包含兩個部分\n",
        "   - text：新聞文字，型態為 TensorFlow Tensor (dtype=tf.string)\n",
        "   - label(標籤)：對應新聞類別，標籤為整數 0~3，依序對應 World、Sports、Business、Sci/Tech，型態為 TensorFlow Tensor (dtype=tf.int64)\n",
        "### 2. 架構\n",
        "1. **資料預處理**  \n",
        "   - 使用Tokenizer將文字轉成數字序列\n",
        "   - 用Embeddings層轉換為向量形式\n",
        "   - 使用 pad_sequences 將序列長度統一\n",
        "2. **模型訓練**  \n",
        "   - 使用Embeding, LSTM, Dense模型進行文本分類  \n",
        "   - 設定損失函數與優化器，訓練模型\n",
        "3. **模型評估**  \n",
        "   - 使用準確率與精確率進行評估\n",
        "     \n",
        "### 3. 預期成果\n",
        "- 模型能對新聞文本自動分類並且達到高準確率\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "t-3wRRROps23"
      },
      "outputs": [],
      "source": [
        "#載入相關套件\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LT2_V55IzqLm"
      },
      "outputs": [],
      "source": [
        "#下載AG news資料集，並且分割成訓練集以及測試集\n",
        "dataset = tfds.load(\n",
        "    \"ag_news_subset\",\n",
        "    as_supervised=True\n",
        ")\n",
        "\n",
        "train_ds = dataset['train']\n",
        "test_ds = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GL3gAKh3qLRm"
      },
      "outputs": [],
      "source": [
        "#將TensorFlow裡的Dataset物件格式的新聞資料轉換為Python list，為後續Tokenizer做使用\n",
        "\n",
        "#訓練資料\n",
        "train_texts = []  #train_texts =  [新聞文字，新聞文字，.....，新聞文字]\n",
        "train_labels = [] #train_labels = [標籤，標籤，.....，標籤]\n",
        "\n",
        "for text, label in train_ds:\n",
        "    train_texts.append(text.numpy().decode('utf-8')) #將原本以tensor物件儲存的文字內容解碼為字串格式\n",
        "    train_labels.append(label.numpy())\n",
        "\n",
        "#測試資料\n",
        "test_texts = []   #test_texts =  [新聞文字，新聞文字，.....，新聞文字]\n",
        "test_labels = []  #test_labels = [標籤，標籤，.....，標籤]\n",
        "\n",
        "for text, label in test_ds:\n",
        "    test_texts.append(text.numpy().decode('utf-8'))  #將原本以tensor物件儲存的文字內容解碼為字串格式\n",
        "    test_labels.append(label.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "pLyRyhZxqPhD"
      },
      "outputs": [],
      "source": [
        "#將文字資料透過Tokenizer轉換為模型可接受的數值序列\n",
        "\n",
        "vocab_size = 10000  # 詞彙量上限\n",
        "max_len = 200       # 最大序列長度\n",
        "\n",
        "#設置Tokenizer，將訓練集裡最多出現次數的詞彙依序編號，最多到10000(vocab_size),超過10000的以<OOV>表示\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "#將訓練集放置在Tokenizer裡面建立詞彙索引\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "#將文字轉換為數值序列\n",
        "#例子: \"Apple releases new iPhone...\" --> [1, 2, 3, 4,...]\n",
        "X_train = tokenizer.texts_to_sequences(train_texts)\n",
        "X_test = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "#透過pad_sequences將所有輸入資料的長度(序列長度)一致(max_len = 200)，不足往前補0\n",
        "#例子:\n",
        "#   原始序列1: [5, 2, 1, ...](假設長度 100)\n",
        "#       pad_sequences 後: [0, 0, ...0, 5, 2, 1,...] (長度 200)\n",
        "#   原始序列2: [7, 3, 8, 2, 9, 4, ...] (假設長度 250)\n",
        "#       pad_sequences 後: [7, 3, 8, 2, 9, 4, ...] (只保留前 200 個元素)\n",
        "x_train = pad_sequences(X_train, maxlen=max_len, padding='pre', truncating='post')\n",
        "x_test = pad_sequences(X_test, maxlen=max_len, padding='pre', truncating='post')\n",
        "\n",
        "# 標籤轉成 numpy array，方便做運算\n",
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "XmyNKfnKqSPU",
        "outputId": "3ef5df8a-a29f-4e4d-cef6-675f9eada1c0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#設置模型，用於新聞主題分類\n",
        "embedding_dim = 64 #Embedding 層的維度，每個詞彙將被表示為 64 維向量\n",
        "#例子:\n",
        "#假設一段文字有'Apple'這個詞彙，經 Embedding 層轉換後會得到一個 64 維向量表示，例如：\n",
        "#'Apple' -> [0.12, -0.03, 0.45, ..., 0.08]  # 共 64 個數字(列表長度為64)\n",
        "#同理，每個詞都會被映射成一個向量，向量中包含詞的語意資訊\n",
        "\n",
        "#使用Sequential來串接神經網路模型\n",
        "#第一層神經網路Embedding層將文字向量化\n",
        "#第二層神經網路LSTM長短期記憶層，用於捕捉序列中詞語的上下文語意\n",
        "#第三層神經網路Dense全連接層，32 個神經元將 LSTM 輸出的語意特徵進行線性組合並加上非線性激活（ReLU），提取更高階的特徵表示。\n",
        "#第四層神經網路Dense全連接層，4 個神經元對應將特徵映射到 4 類新聞，使用 softmax 激活函數輸出各類別機率，模型最終根據最大機率決定新聞分類\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(4, activation='softmax')  # 4 類新聞\n",
        "])\n",
        "\n",
        "#編譯模型，設定損失函數、優化器與評估指標\n",
        "#損失函數用於多分類問題，當標籤是整數形式（0~3）時使用 sparse_categorical_crossentropy\n",
        "#優化器Adam是一種自適應學習率的梯度下降方法，收斂快且穩定\n",
        "#訓練與測試時會計算準確率 (accuracy) 作為模型性能參考\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "#輸出模型的完整架構\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "YyaB-4CsqTDd"
      },
      "outputs": [],
      "source": [
        "#為了避免避免模型學到「順序」而不是「內容」，因此需要使用shuffle隨機打亂資料順序，所以需要先將文字序列與標籤轉換成TensorFlow Dataset以便操作\n",
        "batch_size = 64 #將資料分成每批64筆進行訓練或測試，總共需要1875批(資料大小為120000筆，因此120000/64=1875)\n",
        "\n",
        "#把X_train和y_train變回TensorFlow Dataset 物件\n",
        "train_ds_tf = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "#自動在內部為你建立一個大小為10000筆資料的緩衝區(由於資料過大因此需要緩衝區來減少記憶體)，用來隨機抽資料(每批64筆資料)\n",
        "train_ds_tf = train_ds_tf.shuffle(10000).batch(batch_size)\n",
        "\n",
        "#把X_test和y_test變回TensorFlow Dataset 物件\n",
        "test_ds_tf = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_ds_tf = test_ds_tf.batch(batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM3NHBiSqWKH",
        "outputId": "d03599e9-ef32-4d70-b157-fd7508514df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 11ms/step - accuracy: 0.7546 - loss: 0.6025 - val_accuracy: 0.9030 - val_loss: 0.2910\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 11ms/step - accuracy: 0.9128 - loss: 0.2564 - val_accuracy: 0.9012 - val_loss: 0.2882\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9280 - loss: 0.2078 - val_accuracy: 0.9063 - val_loss: 0.2906\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9398 - loss: 0.1730 - val_accuracy: 0.9014 - val_loss: 0.3139\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9495 - loss: 0.1413 - val_accuracy: 0.9028 - val_loss: 0.3603\n"
          ]
        }
      ],
      "source": [
        "#負責訓練模型並記錄訓練歷史\n",
        "epochs = 5 #表示整個訓練資料將被模型完整看 5 遍\n",
        "\n",
        "# 訓練模型並記錄訓練過程\n",
        "history = model.fit(\n",
        "    train_ds_tf, #訓練資料（已經 shuffle + batch）\n",
        "    validation_data=test_ds_tf,# 驗證資料，用於每個 epoch 後評估模型準確率\n",
        "    epochs=epochs# 訓練的總輪數\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFbxikaTpFGc"
      },
      "source": [
        "# 模型實測\n",
        "使用python套件newspaper3k抓取BBC新聞內容放進模型裡進行分類\n",
        "## 使用新聞內容\n",
        "### 1.體育新聞\n",
        "- 網址: https://www.bbc.com/sport/basketball/articles/c8dyzyj9d88o\n",
        "\n",
        "### 2.科學新聞\n",
        "- 網址: https://www.bbc.com/news/articles/cd6xl3ql3v0o\n",
        "\n",
        "### 3.金融新聞\n",
        "- 網址: https://www.bbc.com/news/articles/cd74lyr094vo\n",
        "\n",
        "### 4.科學新聞\n",
        "- 網址: https://www.bbc.com/future/article/20251023-how-hydrofoil-boats-could-cut-emissions-from-water-transport\n",
        "\n",
        "### 5.世界新聞\n",
        "- 網址: https://www.bbc.com/culture/article/20251223-the-salt-path-and-2025s-most-scandalous-books\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sduIPUlVdfIX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install newspaper3k\n",
        "!pip install lxml[html_clean]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "5T-Jx1sud2qI"
      },
      "outputs": [],
      "source": [
        "#抓取新聞文章\n",
        "from newspaper import Article\n",
        "\n",
        "news = []\n",
        "titles = []\n",
        "\n",
        "#體育新聞\n",
        "url = 'https://www.bbc.com/sport/basketball/articles/c8dyzyj9d88o'\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "text1 = article.text  # 內文\n",
        "news.append(text1)\n",
        "titles.append(article.title)\n",
        "\n",
        "#科技新聞\n",
        "url = 'https://www.bbc.com/news/articles/cd6xl3ql3v0o'\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "text2 = article.text  # 內文\n",
        "news.append(text2)\n",
        "titles.append(article.title)\n",
        "\n",
        "#金融新聞\n",
        "url = 'https://www.bbc.com/news/articles/cd74lyr094vo'\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "text3 = article.text  # 內文\n",
        "news.append(text3)\n",
        "titles.append(article.title)\n",
        "\n",
        "#科學新聞\n",
        "url = 'https://www.bbc.com/future/article/20251023-how-hydrofoil-boats-could-cut-emissions-from-water-transport'\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "text4 = article.text  # 內文\n",
        "news.append(text4)\n",
        "titles.append(article.title)\n",
        "\n",
        "#世界新聞(文化)\n",
        "url = 'https://www.bbc.com/culture/article/20251223-the-salt-path-and-2025s-most-scandalous-books'\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "text5 = article.text  # 內文\n",
        "news.append(text5)\n",
        "titles.append(article.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFooTgMrqXp-",
        "outputId": "4d761eaf-6962-45ed-cd38-f870605428cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "新聞: Nikola Jokic breaks Steph Curry record with historic triple-double in Denver Nuggets win\n",
            "預測主題: Sports\n",
            "\n",
            "新聞: One in three using AI for emotional support and conversation, UK says\n",
            "預測主題: Sci/Tech\n",
            "\n",
            "新聞: US pauses offshore wind projects over national security concerns\n",
            "預測主題: Business\n",
            "\n",
            "新聞: 'The sound completely changes': To electrify boats, make them fly\n",
            "預測主題: Sci/Tech\n",
            "\n",
            "新聞: The Salt Path and 2025's most scandalous books\n",
            "預測主題: World\n",
            "\n"
          ]
        }
      ],
      "source": [
        "seq = tokenizer.texts_to_sequences(news)\n",
        "padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
        "\n",
        "#放進模型做分類\n",
        "pred = model.predict(padded)\n",
        "labels = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "\n",
        "# print(pred)\n",
        "\n",
        "#輸出分類結果\n",
        "for title, p in zip(titles, pred):\n",
        "    pred_class = labels[np.argmax(p)]\n",
        "    print(f\"新聞: {title}\")\n",
        "    print(f\"預測主題: {pred_class}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLlFrCKAMudCoRi5Myo7mV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}